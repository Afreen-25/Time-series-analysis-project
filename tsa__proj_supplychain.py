# -*- coding: utf-8 -*-
"""TSA _proj_supplychain.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1O2B6fRxKRojzi6fig_dB6X9ce4fuycAz

In Retail Industry and chain of stores one of the biggest issue they face are supply chain management. The component of supply chain management (SCM) involved with determining how best to fulfill the requirements created from the Demand Plan.

It's objective is to balance supply and demand in a manner that achieves the financial and service objectives of the enterprise.

# Data

There are 3 Datasets :

Stores:
*   Store: The store number. Range from 1–45.
*   Type: Three types of stores ‘A’, ‘B’ or ‘C’.
*   Size: Sets the size of a Store would be calculated by the no. of products   
    available in the particular store ranging from 34,000 to 210,000
*   primary key is Store

Sales:
*   Date: The date of the week where this observation was taken.
*   Weekly_Sales: The sales recorded during that Week.
*   Store: The store which observation in recorded 1–45
*   Dept: One of 1–99 that shows the department.
*   IsHoliday: Boolean value representing a holiday week or not.
*   primary key is a combination of (Store,Dept,Date).

Features:
*   Temperature: Temperature of the region during that week.
*   Fuel_Price: Fuel Price in that region during that week.
*   MarkDown1:5 : Represents the Type of markdown and what quantity was
    available during that week.
*   CPI: Consumer Price Index during that week.
*   Unemployment: The unemployment rate during that week in the region of the
    store.
*   primary key here is a combination of (Store,Date)
"""

import numpy as np
import pandas as pd
from matplotlib import pyplot as plt
import seaborn as sns
import calendar
from sklearn.model_selection import cross_val_score, cross_val_predict
from sklearn import metrics
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import Lasso, LinearRegression, LassoCV
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.neighbors import KNeighborsRegressor
import random
import sqlite3
from itertools import cycle, islice
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import GradientBoostingRegressor
import xgboost as xgb

import lightgbm as lgb
from sklearn.experimental import enable_hist_gradient_boosting
from sklearn.ensemble import HistGradientBoostingRegressor
from sklearn.svm import SVR

"""Load and read data"""

df=pd.read_csv('/content/features.csv.zip')
sampledf=pd.read_csv('/content/sampleSubmission.csv.zip')
testdf=pd.read_csv('/content/test.csv.zip')
traindf=pd.read_csv('/content/train.csv.zip')
storesdf=pd.read_csv('/content/stores.csv')
sampledf.head()

storesdf.head()

df.head()

testdf.head()

traindf.head()

traindf.shape

"""#Group by Store

Since we are predicting the weekly sales for Store level, we will grouping the data in walmart dataset to avoid the department and take the sum of department sales to store level.
"""

storegroupdf=traindf.groupby(["Store","Date"])[["Weekly_Sales"]].sum()
storegroupdf.reset_index(inplace=True)

"""Merging all the datasets into one place for easier test and analysis."""

result = pd.merge(storegroupdf, storesdf, how='inner', on='Store', left_on=None, right_on=None,
        left_index=False, right_index=False, sort=False,
        suffixes=('_x', '_y'), copy=True, indicator=False)

data = pd.merge(result, df, how='inner', on=['Store','Date'], left_on=None, right_on=None,
        left_index=False, right_index=False, sort=False,
        suffixes=('_x', '_y'), copy=True, indicator=False)

print(data.shape)

"""Dataframe Walmart with 421570 rows has come down to 6435 rows by doing a group by and merge

#Data Cleaning
"""

data.head()

data['IsHoliday'] = data['IsHoliday'].apply(lambda x: 1 if x == True else 0)

data.dtypes

data["Date"]=pd.to_datetime(data.Date)


data["Day"]=data.Date.dt.day
data["Month"]=data.Date.dt.month
data["Year"]=data.Date.dt.year

data['Month'] = data['Month'].apply(lambda x: calendar.month_abbr[x])

data.isnull().sum()

data.describe().T

data['Week'] = data.Date.dt.isocalendar().week

"""#EDA"""

df.describe().T

df_weeks = data.groupby('Week').sum()

import plotly.express as px
fig = px.line( data_frame = df_weeks, x = df_weeks.index, y = 'Weekly_Sales', labels = {'Weekly_Sales' : 'Weekly Sales', 'x' : 'Weeks' }, title = 'Sales over weeks')
fig.update_traces(line_color='deeppink', line_width=3)

df_weeks.head()

data.describe().T.style.bar(subset=['mean'], color='#205ff2')\
                            .background_gradient(subset=['std'], cmap='Reds')\
                            .background_gradient(subset=['50%'], cmap='coolwarm')

data['MarkDown1'].fillna(-500, inplace=True)
data['MarkDown2'].fillna(-500, inplace=True)
data['MarkDown3'].fillna(-500, inplace=True)
data['MarkDown4'].fillna(-500, inplace=True)
data['MarkDown5'].fillna(-500, inplace=True)

plt.figure(figsize=(10, 6))
data["Weekly_Sales"]=data.Weekly_Sales/1000

sns.distplot(data.Weekly_Sales, kde=False, bins=30, color = 'tomato')
plt.title('Weekly Sales Function Distribution')
plt.show()

"""In the Distribution, natural Log of Sales and the square root of Sales look better distributed. We can use Natural Log for predictions later"""

def scatter(dataset, column):
    plt.figure()
    plt.scatter(data[column] , data['Weekly_Sales'], color = 'turquoise')
    plt.ylabel('Weekly Sales')
    plt.xlabel(column)

scatter(data, 'Fuel_Price')
scatter(data, 'Size')
scatter(data, 'CPI')
scatter(data, 'Type')
scatter(data, 'IsHoliday')
scatter(data, 'Unemployment')
scatter(data, 'Temperature')
scatter(data, 'Store')

data['Week'] = data.Date.dt.isocalendar().week

data.head()

weekly_sales_mean = data['Weekly_Sales'].groupby(data['Date']).mean()
weekly_sales_median = data['Weekly_Sales'].groupby(data['Date']).median()
plt.figure(figsize=(20,8))
sns.lineplot(x=weekly_sales_mean.index, y=weekly_sales_mean.values, color = 'indigo')
sns.lineplot(x=weekly_sales_median.index,y= weekly_sales_median.values, color = 'tomato')
plt.grid()
plt.legend(['Mean', 'Median'], loc='best', fontsize=16)
plt.title('Weekly Sales - Mean and Median', fontsize=18)
plt.ylabel('Sales', fontsize=16)
plt.xlabel('Date', fontsize=16)
plt.show()

"""#Checking the relationship of the other features with weekly sales"""

plt.figure(figsize=(18,8))
sns.lineplot ( data = data, x = 'Size', y =  'Weekly_Sales', hue = 'IsHoliday');

px.histogram(data, x='Temperature', y ='Weekly_Sales', color='IsHoliday', marginal='box')

px.histogram(data, x='Fuel_Price', y ='Weekly_Sales', color='IsHoliday', marginal='box')

px.histogram(data, x='CPI', y ='Weekly_Sales', color='IsHoliday')

weekly_sales = data['Weekly_Sales'].groupby(data['Store']).mean()
plt.figure(figsize=(20,8))
plt.style.use('default')
sns.barplot(x=weekly_sales.index, y=weekly_sales.values)
plt.grid()
plt.title('Average Sales - per Store', fontsize=18)
plt.ylabel('Sales', fontsize=16)
plt.xlabel('Store', fontsize=16)
plt.show()

sns.set(style="white")

corr = data.corr()
mask = np.triu(np.ones_like(corr, dtype=np.bool))
f, ax = plt.subplots(figsize=(20, 15))
cmap = sns.diverging_palette(220, 10, as_cmap=True)
plt.title('Correlation Matrix', fontsize=18)
sns.heatmap(corr, mask=mask, cmap=cmap, vmax=1, center=0,
            square=True, linewidths=.5, cbar_kws={"shrink": .5}, annot=True)

plt.show()

"""#Detailed Time-Series Analysis

Store 4 & Store 6
"""

data1 = pd.read_csv('/content/train.csv.zip')
data1.set_index('Date', inplace=True)

store4 = data1[data1.Store == 4]
store6 = data1[data1.Store == 6]

sales4 = pd.DataFrame(store4.Weekly_Sales.groupby(store4.index).sum())
sales6 = pd.DataFrame(store6.Weekly_Sales.groupby(store6.index).sum())
sales4.dtypes
sales4.head(20)
sales6.dtypes
sales6.head(20)
sales4.reset_index(inplace = True)
sales6.reset_index(inplace = True)
sales4['Date'] = pd.to_datetime(sales4['Date'])
sales6['Date'] = pd.to_datetime(sales6['Date'])
sales4.set_index('Date',inplace = True)
sales6.set_index('Date',inplace = True)

sales4.Weekly_Sales.plot(figsize=(10,6), title= 'Weekly Sales of Store1', fontsize=14, color = 'salmon')
plt.show()

"""#Time Series Components

Trend: It represents the change in dependent variables with respect to time from start to end.

Seasonality: If observations repeats after fixed time interval then they are referred as seasonal observations.

Irregularities: This is also called as noise. Strange dips and jump in the data are called as irregularities.

Cyclicity: It occurs when observations in the series repeats in random pattern. Note that if there is any fixed pattern then it becomes seasonality, in case of cyclicity observations may repeat after a week, months or may be after a year.

---
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
# %matplotlib inline
from statsmodels.tsa.filters.hp_filter import hpfilter
df = pd.read_csv('/content/train.csv.zip', index_col=0, parse_dates = True)
Weekly_Sales_cycle, Weekly_Sales_trend = hpfilter(df['Weekly_Sales'], lamb=1600)
Weekly_Sales_trend.plot(figsize=(15,6)).autoscale(axis='x',tight=True)

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import matplotlib.pyplot as plt
from scipy import signal
import warnings
warnings.filterwarnings("ignore")
# %matplotlib inline
df = pd.read_csv('/content/train.csv.zip',index_col=0, parse_dates = True)
detrended = signal.detrend(df.Weekly_Sales.values)
plt.figure(figsize = (15,6))
plt.plot(detrended)
plt.xlabel('Weekly Sales')
plt.ylabel('Frequency')
plt.title('Detrending using Scipy Signal',fontsize = 16)
plt.show()

from pandas import read_excel
from matplotlib import pyplot
import warnings
warnings.filterwarnings("ignore")
df = pd.read_csv('/content/train.csv.zip', index_col=0, parse_dates = True)
diff = df.Weekly_Sales.diff()
plt.figure(figsize = (15,6))
pyplot.plot(diff)
pyplot.title('Detrending using Pandas Differencing', fontsize =16)
pyplot.xlabel('Week')
pyplot.ylabel('Weekly_Sales')
pyplot.show()

from pandas import read_excel
from matplotlib import pyplot
import warnings
warnings.filterwarnings("ignore")
df = pd.read_csv('/content/train.csv.zip', index_col=0, parse_dates = True)
Weekly_Sales_cycle, Weekly_Sales_trend = hpfilter(df['Weekly_Sales'], lamb=1600)
df['trend']= Weekly_Sales_trend
detrended= df.Weekly_Sales -df['trend']
plt.figure(figsize = (15,6))
pyplot.plot(detrended)
pyplot.title('Detrending using HP filter', fontsize =16)
pyplot.xlabel('Week')
pyplot.ylabel('Weekly_Sales')
pyplot.show()

"""Additive Decomposition




*   An additive model suggests that the components are added together.

*   An additive model is linear where changes over time are consistently made  
    by the same amount.
*   A linear seasonality has the same frequency (width of the cycles) and      amplitude (height of the cycles).








The statsmodels library provides an implementation of the naive, or classical, decomposition method in a function called seasonal_decompose(). You need to specify whether the model is additive or multiplicative.

The seasonal_decompose() function returns a result object which contains arrays to access four pieces of data from the decomposition.
"""

from statsmodels.tsa.seasonal import seasonal_decompose

decomposition = seasonal_decompose(sales4.Weekly_Sales, period=12)
fig = plt.figure()
fig = decomposition.plot()
fig.set_size_inches(12, 10)
plt.show()

"""Multiplicative Decomposition


*   An additive model suggests that the components are multipled together.

*   An additive model is non-linear such as quadratic or exponential.
*   Changes increase or decrease over time.


*   A non-linear seasonality has an increasing or decreasing frequency (width of the cycles) and / or amplitude (height of the cycles) over time.






"""

decomposition = seasonal_decompose(sales4.Weekly_Sales, model= 'multiplicative', period=12)
fig = plt.figure()
fig = decomposition.plot()
fig.set_size_inches(12, 10)
plt.show()

y1=sales4.Weekly_Sales
y2=sales6.Weekly_Sales

y1.plot(figsize=(15, 6), legend=True, color = 'turquoise')
y2.plot(figsize=(15, 6), legend=True, color = 'salmon')
plt.ylabel('Weekly Sales')
plt.title('Store4 vs Store6 Weekly Sales', fontsize = '16')
plt.show()

"""This shows an interesting trend during year ends (during both 2011 & 2012). The best thing is both the stores have almost the same trends and spike just the magnitude is different."""

# Lets Look into 2012 data for a better view
y1['2012'].plot(figsize=(15, 6),legend=True, color = 'chocolate')
y2['2012'].plot(figsize=(15, 6), legend=True, color = 'turquoise')
plt.ylabel('Weekly Sales')
plt.title('Store4 vs Store6 on 2012', fontsize = '16')
plt.show()

"""#ADF TEST"""

import pandas as pd
from matplotlib import pyplot
from statsmodels.tsa.stattools import adfuller

series = pd.read_csv(r'/content/features.csv.zip', header=0, index_col=0)
series['Temperature'].plot(figsize=(15,6))
pyplot.show()

def Augmented_Dickey_Fuller_Test_func(series , column_name):
    print (f'Results of Dickey-Fuller Test for column: {column_name}')
    dftest = adfuller(series, autolag='AIC')

    dfoutput = pd.Series(dftest[0:4], index=['Test Statistic', 'p-value', 'Lags Used', 'Number of Observations Used'])
    for key,value in dftest[4].items():
       dfoutput['Critical Value (%s)'%key] = value
    print (dfoutput)
    if dftest[1] <= 0.05:
        print("Conclusion:====>")
        print("Reject the null hypothesis")
        print("Data is stationary")
    else:
        print("Conclusion:====>")
        print("Fail to reject the null hypothesis")
        print("Data is non-stationary")

Augmented_Dickey_Fuller_Test_func( series['Temperature'], 'Temperature')

series['Temperature'].diff().plot(figsize=(15,6))

Augmented_Dickey_Fuller_Test_func(series['Temperature'].diff().dropna(),'')

"""#ARIMA MODELLING"""

import itertools
p = d = q = range(0, 5)
pdq = list(itertools.product(p, d, q))
seasonal_pdq = [(x[0], x[1], x[2], 52) for x in list(itertools.product(p, d, q))]

import statsmodels.api as sm

mod = sm.tsa.statespace.SARIMAX(y1,
                                order=(4, 4, 3),
                                seasonal_order=(1, 1, 0, 52),   #enforce_stationarity=False,
                                enforce_invertibility=False)

results = mod.fit()

print(results.summary().tables[1])

plt.style.use('seaborn-pastel')
results.plot_diagnostics(figsize=(15, 12))
plt.show()

# Will predict for last 90 days. So setting the date according to that
pred = results.get_prediction(start=pd.to_datetime('2012-07-27'), dynamic=False)
pred_ci = pred.conf_int()

ax = y1['2010':].plot(label='observed')
pred.predicted_mean.plot(ax=ax, label='One-step ahead Forecast', alpha=.7)

ax.fill_between(pred_ci.index,
                pred_ci.iloc[:, 0],
                pred_ci.iloc[:, 1], color='k', alpha=.2)

ax.set_xlabel('Time Period')
ax.set_ylabel('Sales')
plt.legend()

plt.show()

y_forecasted = pred.predicted_mean
y_truth = y1['2012-7-27':]

# Compute the mean square error
mse = ((y_forecasted - y_truth) ** 2).mean()
print('The Mean Squared Error of our forecasts is {}'.format(round(mse, 2)))

pred_dynamic = results.get_prediction(start=pd.to_datetime('2012-7-27'), dynamic=True, full_results=True)
pred_dynamic_ci = pred_dynamic.conf_int()
ax = y1['2010':].plot(label='observed', figsize=(12, 8))
pred_dynamic.predicted_mean.plot(label='Dynamic Forecast', ax=ax)

ax.fill_between(pred_dynamic_ci.index,
                pred_dynamic_ci.iloc[:, 0],
                pred_dynamic_ci.iloc[:, 1], color='k', alpha=.25)

ax.fill_betweenx(ax.get_ylim(), pd.to_datetime('2012-7-26'), y1.index[-1],
                 alpha=.1, zorder=-1)

ax.set_xlabel('Time Period')
ax.set_ylabel('Sales')

plt.legend()
plt.show()

"""That looks good. Both the observed and predicted lines go together indicating nearly accurate prediction"""

import numpy as np
y_forecasted = pred_dynamic.predicted_mean

y_truth = y1['2012-7-27':]

rmse = np.sqrt(((y_forecasted - y_truth) ** 2).mean())
print('The Root Mean Squared Error of our forecasts is {}'.format(round(rmse, 2)))

Residual= y_forecasted - y_truth
print("Residual for Store1",np.abs(Residual).sum())

pred_uc = results.get_forecast(steps=12)

pred_ci = pred_uc.conf_int()
ax = y1.plot(label='observed', figsize=(12, 8))
pred_uc.predicted_mean.plot(ax=ax, label='Forecast')
ax.fill_between(pred_ci.index,
                pred_ci.iloc[:, 0],
                pred_ci.iloc[:, 1], color='k', alpha=.25)
ax.set_xlabel('Time Period')
ax.set_ylabel('Sales')

plt.legend()
plt.show()

"""For future prediction the model is not that great because the error interval is way big. But if we just check the green line prediction this is almost like earlier years. If we look for may be first 2 weeks the prediction is way better and error is also low."""

!pip install pmdarima

import warnings

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import statsmodels.api as sm
from pmdarima import auto_arima
from sklearn import metrics
from statsmodels.tsa.seasonal import seasonal_decompose
from statsmodels.tsa.stattools import adfuller

warnings.filterwarnings("ignore")

X = df[['Weekly_Sales']]
train, test = X[0:-30], X[-30:]

stepwise_model = auto_arima(train,start_p=1, start_q=1,
    max_p=2, max_q=2, seasonal=False,
    d=None, trace=True,error_action='ignore',suppress_warnings=True, stepwise=True)

stepwise_model.summary()

forecast,conf_int = stepwise_model.predict(n_periods=30,return_conf_int=True)
forecast = pd.DataFrame(forecast,columns=['sales_pred'])

df_conf = pd.DataFrame(conf_int,columns= ['Upper_bound','Lower_bound'])
df_conf["new_index"] = range(1, 31)
df_conf = df_conf.set_index("new_index")

forecast["new_index"] = range(1,31)
forecast = forecast.set_index("new_index")

# Commented out IPython magic to ensure Python compatibility.
import matplotlib.pyplot as plt
# %matplotlib inline
plt.rcParams["figure.figsize"] = [15,7]
plt.plot( train, label='Train ')
plt.plot(test, label='Test ')
plt.plot(forecast, label='Predicted ')
plt.plot(df_conf['Upper_bound'], label='Confidence Interval Upper bound ')
plt.plot(df_conf['Lower_bound'], label='Confidence Interval Lower bound ')
plt.legend(loc='best')
plt.show()

stepwise_model.plot_diagnostics()

"""#Smoothing Method"""

from statsmodels.tsa.api import SimpleExpSmoothing
import pandas as pd
import numpy as np
from sklearn import metrics
df =pd.read_csv(r'/content/features.csv.zip', header=0, index_col=0)

df.head()

#Making a copy of the data for Close column for train/split
X = df['Temperature']
test = X.iloc[-30:] #Data expected for last 30 days
train = X.iloc[:-30] #Last 30 days to evaluate prediction

#Function for calculating evaluation metrics to understand how far off our forecasts are against the actuals
#Closer to 0 better the model
#RMSE and MAPE are notable statistics measures used to check the accurcy of forecasting model.

def timeseries_evaluation_metrics_func(y_true, y_pred):

    def mean_absolute_percentage_error(y_true, y_pred):
        y_true, y_pred = np.array(y_true), np.array(y_pred)
        return np.mean(np.abs((y_true - y_pred) / y_true)) * 100
    print('Evaluation metric results:-')
    print(f'MSE is : {metrics.mean_squared_error(y_true, y_pred)}') #how close a regression line is to a set of points.
    print(f'MAE is : {metrics.mean_absolute_error(y_true, y_pred)}') # average magnitude of the errors in a set of predictions.
    print(f'RMSE is : {np.sqrt(metrics.mean_squared_error(y_true, y_pred))}') #square root of the avg of squared diff bet pred and actual obs
    print(f'MAPE is : {mean_absolute_percentage_error(y_true, y_pred)}') #statistical measure of how accurate a forecast is.
    print(f'R2 is : {metrics.r2_score(y_true, y_pred)}',end='\n\n') #proportion of variance in dependent variable by the independent variable

resu = []
temp_df = pd.DataFrame()
for i in [0 , 0.10, 0.20, 0.30, 0.40, 0.50, 0.60, 0.70, 0.80, 0.90,1]:
    print(f'Fitting for smoothing level= {i}')
    fit_v = SimpleExpSmoothing(np.asarray(train)).fit(i)
    fcst_pred_v= fit_v.forecast(30)
    timeseries_evaluation_metrics_func(test,fcst_pred_v)
    rmse = np.sqrt(metrics.mean_squared_error(test, fcst_pred_v))
    df3 = {'smoothing parameter':i, 'RMSE': rmse}
    temp_df = temp_df.append(df3, ignore_index=True)
temp_df.sort_values(by=['RMSE'])

fitSES = SimpleExpSmoothing(np.asarray(train)).fit( smoothing_level = 1.0,optimized= False)
fcst_gs_pred = fitSES.forecast(30)
timeseries_evaluation_metrics_func(test,fcst_gs_pred)

fitSESauto = SimpleExpSmoothing(np.asarray(train)).fit( optimized= True, use_brute = True)
fcst_auto_pred = fitSESauto.forecast(30)
timeseries_evaluation_metrics_func(test,fcst_auto_pred)

fitSESauto.summary()

df_fcst_gs_pred = pd.DataFrame(fcst_gs_pred, columns=['Close_grid_Search'])
df_fcst_gs_pred["new_index"] = range(1229, 1259)
df_fcst_gs_pred = df_fcst_gs_pred.set_index("new_index")

df_fcst_auto_pred = pd.DataFrame(fcst_auto_pred, columns=['Close_auto_search'])
df_fcst_auto_pred["new_index"] = range(1229, 1259)
df_fcst_auto_pred = df_fcst_auto_pred.set_index("new_index")

# Commented out IPython magic to ensure Python compatibility.
import matplotlib.pyplot as plt
# %matplotlib inline
plt.rcParams["figure.figsize"] = [16,9]
plt.plot( train, label='Train')
plt.plot(test, label='Test')
plt.plot(df_fcst_auto_pred, label='Simple Exponential Smoothing using optimized =True')
plt.plot(df_fcst_gs_pred, label='Simple Exponential Smoothing using custom grid search')
plt.legend(loc='best')
plt.show()

import pandas as pd
import numpy as np
from sklearn import metrics
from timeit import default_timer as timer
from statsmodels.tsa.api import ExponentialSmoothing, SimpleExpSmoothing, Holt

df.head()

df['Temperature'] = df['Temperature']

train = df.Temperature[0:-30]
test = df.Temperature[-30:]

def timeseries_evaluation_metrics_func(y_true, y_pred):

    def mean_absolute_percentage_error(y_true, y_pred):
        y_true, y_pred = np.array(y_true), np.array(y_pred)
        return np.mean(np.abs((y_true - y_pred) / y_true)) * 100
    print('Evaluation metric results:-')
    print(f'MSE is : {metrics.mean_squared_error(y_true, y_pred)}')
    print(f'MAE is : {metrics.mean_absolute_error(y_true, y_pred)}')
    print(f'RMSE is : {np.sqrt(metrics.mean_squared_error(y_true, y_pred))}')
    print(f'MAPE is : {mean_absolute_percentage_error(y_true, y_pred)}')
    print(f'R2 is : {metrics.r2_score(y_true, y_pred)}',end='\n\n')

from sklearn.model_selection import ParameterGrid
#dampening - decreasing future time trends on a straight line(no trend)
param_grid = {'smoothing_level': [0.10, 0.20,.30,.40,.50,.60,.70,.80,.90], 'smoothing_slope':[0.10, 0.20,.30,.40,.50,.60,.70,.80,.90],
              'damping_slope': [0.10, 0.20,.30,.40,.50,.60,.70,.80,.90],'damped' : [True, False]}
pg = list(ParameterGrid(param_grid))

df_results_moni = pd.DataFrame(columns=['smoothing_level', 'smoothing_slope', 'damping_slope','damped','RMSE','r2'])
start = timer()
for a,b in enumerate(pg):
    smoothing_level = b.get('smoothing_level')
    smoothing_slope = b.get('smoothing_slope')
    damping_slope = b.get('damping_slope')
    damped = b.get('damped')
    print(smoothing_level, smoothing_slope, damping_slope,damped)
    fit1 = Holt(train,damped =damped ).fit(smoothing_level=smoothing_level, smoothing_slope=smoothing_slope, damping_slope = damping_slope ,optimized=False)
    #fit1.summary
    z = fit1.forecast(30)
    print(z)
    df_pred = pd.DataFrame(z, columns=['Forecasted_result'])
    RMSE = np.sqrt(metrics.mean_squared_error(test, df_pred.Forecasted_result))
    r2 = metrics.r2_score(test, df_pred.Forecasted_result)
    print( f' RMSE is {np.sqrt(metrics.mean_squared_error(test, df_pred.Forecasted_result))}')
    df_results_moni = df_results_moni.append({'smoothing_level' :smoothing_level, 'smoothing_slope':smoothing_slope, 'damping_slope' :damping_slope,'damped':damped,'RMSE': RMSE,'r2':r2}, ignore_index=True)
end = timer()
print(f' Total time taken to complete grid search in seconds: {(end - start)}')

print(f' Below mentioned parameter gives least RMSE and r2')
df_results_moni.sort_values(by=['RMSE','r2']).head(1)

it1 = Holt(train,damped =False ).fit(smoothing_level=0.9, smoothing_slope=0.6, damping_slope = 0.1 ,optimized=False)

Forecast_custom_pred = fit1.forecast(30)

fit1.summary()

timeseries_evaluation_metrics_func(test,Forecast_custom_pred)

# Automated Parameter
fitESAUTO = Holt(train).fit(optimized= True, use_brute = True)
fitESAUTO.summary()

fitESAUTOpred = fitESAUTO.forecast(30)
timeseries_evaluation_metrics_func(test,fitESAUTOpred)

# Commented out IPython magic to ensure Python compatibility.
import matplotlib.pyplot as plt
# %matplotlib inline
plt.rcParams["figure.figsize"] = [16,9]
plt.plot( train, label='Train')
plt.plot(test, label='Test')
plt.plot(fitESAUTOpred, label='Automated grid search')
plt.plot(Forecast_custom_pred, label='Double Exponential Smoothing with custom grid search')
plt.legend(loc='best')
plt.show()

"""#VAR

"""

import pandas as pd
import numpy as np
from sklearn import metrics
df =pd.read_csv(r'/content/features.csv (1).zip', header=0, index_col=0)

import pandas as pd
import matplotlib.pyplot as plt
from statsmodels.tsa.api import VAR
import numpy as np
from statsmodels.tsa.stattools import adfuller
from sklearn import metrics
from timeit import default_timer as timer
import warnings
warnings.filterwarnings("ignore")

df.head()

df = df[(df['Date'] > '2012-01-14') & (df['Date'] <= '2013-01-30')]

df.head()

df.shape

df.columns

for c in df[[ 'Temperature', 'Fuel_Price', 'MarkDown1', 'MarkDown2']]:
    df[str(c)].plot(figsize=(15, 6))
    plt.xlabel("Date")
    plt.ylabel(c)
    plt.title(" walmart stocks before stationary")
    plt.show()

for c in df[['Temperature', 'Fuel_Price', 'MarkDown1', 'MarkDown2']]:
    plt.figure(1, figsize=(15,6))
    plt.subplot(211)
    plt.title(f"{str(c)} Histogram before stationary")
    df[str(c)].hist()
    plt.subplot(212)
    df[str(c)].plot(kind='kde')
    plt.title(f"{str(c)} Kernal Density Estimator before stationary")
    plt.show()

def timeseries_evaluation_metrics_func(y_true, y_pred):

    def mean_absolute_percentage_error(y_true, y_pred):
        y_true, y_pred = np.array(y_true), np.array(y_pred)
        return np.mean(np.abs((y_true - y_pred) / y_true)) * 100
    #print('Evaluation metric results:-')
    print(f'MSE is : {metrics.mean_squared_error(y_true, y_pred)}')
    print(f'MAE is : {metrics.mean_absolute_error(y_true, y_pred)}')
    print(f'RMSE is : {np.sqrt(metrics.mean_squared_error(y_true, y_pred))}')
    print(f'MAPE is : {mean_absolute_percentage_error(y_true, y_pred)}')
    print(f'R2 is : {metrics.r2_score(y_true, y_pred)}',end='\n\n')
    return

def Augmented_Dickey_Fuller_Test_func(series , column_name):
    print (f'Results of Dickey-Fuller Test for column: {column_name}')
    dftest = adfuller(series, autolag='AIC')
    dfoutput = pd.Series(dftest[0:4], index=['Test Statistic','p-value','No Lags Used','Number of Observations Used'])
    for key,value in dftest[4].items():
       dfoutput['Critical Value (%s)'%key] = value
    print (dfoutput)
    if dftest[1] <= 0.05:
        print("Conclusion:====>")
        print("Reject the null hypothesis")
        print("Data is stationary")
    else:
        print("Conclusion:====>")
        print("Fail to reject the null hypothesis")
        print("Data is non-stationary")

for name, column in df[['Temperature', 'Fuel_Price']].iteritems():
    Augmented_Dickey_Fuller_Test_func(df[name],name)
    print('\n')

X = df[['Temperature', 'Fuel_Price', 'MarkDown1', 'MarkDown2' ]]
train, test = X[0:-30], X[-30:]

train_diff = train.diff()
train_diff.dropna(inplace = True)

for name, column in train_diff[[ 'Temperature', 'Fuel_Price', 'MarkDown1', 'MarkDown2']].iteritems():
    Augmented_Dickey_Fuller_Test_func(train_diff[name],name)
    print('\n')

for c in train_diff[['Temperature', 'Fuel_Price', 'MarkDown1', 'MarkDown2']]:
    train_diff[str(c)].plot(figsize=(15, 6))
    plt.xlabel("Date")
    plt.ylabel(c)
    plt.title("walmart stocks after stationary")
    plt.show()

for c in train_diff[['Temperature', 'Fuel_Price', 'MarkDown1', 'MarkDown2']]:
    plt.figure(1, figsize=(15,6))
    plt.subplot(211)
    plt.title(f"{str(c)} Histogram after stationary")
    train_diff[str(c)].hist()
    plt.subplot(212)
    train_diff[str(c)].plot(kind='kde')
    plt.title(f"{str(c)} Kernal Density Estimator after stationary")
    plt.show()

train_diff.head(10)

from statsmodels.tsa.vector_ar.vecm import coint_johansen

def cointegration_test(df):
    res = coint_johansen(df,-1,5)
    d = {'0.90':0, '0.95':1, '0.99':2}
    traces = res.lr1 # Trace statistics
    cvts = res.cvt[:, d[str(1-0.05)]] #Critical values (90%, 95%, 99%) for trace statistic.
    def adjust(val, length= 6):
        return str(val).ljust(length) #string left justified (ljust)
    print('Column Name   >  Test Stat > C(95%)    =>   Signif  \n', '--'*20)
    for col, trace, cvt in zip(df.columns, traces, cvts):
        print(adjust(col), '> ', adjust(round(trace,2), 9), ">", adjust(cvt, 8), ' =>  ' , trace > cvt)

cointegration_test(train_diff[['Temperature', 'Fuel_Price', 'MarkDown1', 'MarkDown2']])

for i in [1,2,3,4,5,6,7,8,9]:
    model = VAR(train_diff)
    results = model.fit(i)
    print(f'Order : {i}, AIC:  {results.aic}, BIC: { results.bic}')

def inverse_diff(actual_df, pred_df):
    df_res = pred_df.copy()
    columns = actual_df.columns
    for col in columns:
        df_res[str(col)+'_1st_inv_diff'] = actual_df[col].iloc[-1] + df_res[str(col)].cumsum()
    return df_res

results = model.fit(4)
display(results.summary())
z = results.forecast(y=train_diff[['Temperature', 'Fuel_Price', 'MarkDown1', 'MarkDown2' ]].values, steps=30)
df_pred = pd.DataFrame(z, columns=['Temperature', 'Fuel_Price', 'MarkDown1', 'MarkDown2'  ])

df_pred["new_index"] = range(233, 263)
df_pred = df_pred.set_index("new_index")

df_pred

res = inverse_diff(df[['Temperature', 'Fuel_Price', 'MarkDown1', 'MarkDown2' ]],df_pred)

res

for i in ['Temperature', 'Fuel_Price' ]:
    print(f'Evaluation metric for {i}')
    timeseries_evaluation_metrics_func(test[str(i)] , res[str(i)+'_1st_inv_diff'])

# Commented out IPython magic to ensure Python compatibility.
import matplotlib.pyplot as plt
# %matplotlib inline
for i in ['Temperature', 'Fuel_Price', 'MarkDown1', 'MarkDown2']:

    plt.rcParams["figure.figsize"] = [10,7]
    plt.plot( train[str(i)], label='Train '+str(i))
    plt.plot(test[str(i)], label='Test '+str(i))
    plt.plot(res[str(i)+'_1st_inv_diff'], label='Predicted '+str(i))
    plt.legend(loc='best')
    plt.show()

"""#CNN"""

import pandas as pd
import numpy as np
from sklearn import metrics
df =pd.read_csv(r'/content/features.csv', header=0, index_col=0)

!pip install keras

import numpy as np
import pandas as pd
from numpy import array
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import Flatten
from keras.layers import Conv1D
from keras.layers import MaxPooling1D

def split_sequence(sequence,steps):
  X,y=list(),list()
  for start in range(len(sequence)):
    end_index = start + steps
    if end_index > len(sequence)-1:
      break
    sequence_x,sequence_y = sequence[start : end_index], sequence[end_index]
    X.append(sequence_x)
    y.append(sequence_y)
  return array(X),array(y)

df.head()

# def exp8(col_name):
#   raw_sequence = df[col_name]
#   # choose number of time steps
#   steps=3
#   # split into samples
#   X,y = split_sequence(raw_sequence,steps)
#   print("****************************************************************")
#   print('The input sequence on which we will test our CNN Model: ')
#   print(raw_sequence)
#   print("\n\n****************************************************************")
#   print('The features (X) , lagged values of time series , for our CNN Model')
#   print(X[:4])
#   print("\n\n****************************************************************")
#   print('The respective observed values (y) of the sequence for training in CNN Model')
#   print(y)
#   print("****************************************************************\n\n")
#   # Reshape 2D imput data to 3D input data
#   features = 1
#   X = X.reshape((X.shape[0], X.shape[1], features))
#   print('After reshaping , the shape of the input X ')
#   print(X.shape)
#   print("\n\n*************************************************")
#   print("*************************************************")
#   print('The final form of input feature matrix X')
#   print(X[:4])
#   print("\n\n*************************************************")
#   print("*************************************************")
#   print('Feature matrix X is ready for input to CNN Model')
#   print("*************************************************\n\n")
#   # Defining model architecture
#   model = Sequential()
#   model.add(Conv1D(filters=64, kernel_size=2, activation='relu', input_shape=(steps,features)))
#   model.add(MaxPooling1D(pool_size=2))
#   model.add(Flatten())
#   model.add(Dense(100, activation='relu'))
#   model.add(Dense(1))
#   model.compile(optimizer='adam', loss='mse')
#   # fit model
#   model.fit(X,y,epochs=100, verbose=0)
#   #predict
#   x_input = array(raw_sequence[-steps-1:-1])
#   x_input = x_input.reshape((1,steps,features))
#   y_pred = model.predict(x_input,verbose=0)
#   print("*************************************************")
#   print(f"The next predicted value is ")
#   print(y_pred)
#   print(f"The next true value is ")
#   print(raw_sequence.iloc[-1])
#   print("*************************************************")
from sklearn.model_selection import train_test_split
col_name = 'Temperature'
raw_sequence = df[col_name]

# Choose number of time steps
steps = 3

# Split into samples
X, y = split_sequence(raw_sequence, steps)

# Reshape 2D input data to 3D input data
features = 1
X = X.reshape((X.shape[0], X.shape[1], features))

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Build the CNN model
model = Sequential()
model.add(Conv1D(filters=64, kernel_size=2, activation='relu', input_shape=(steps, features)))
model.add(MaxPooling1D(pool_size=2))
model.add(Flatten())
model.add(Dense(100, activation='relu'))
model.add(Dense(1))
model.compile(optimizer='adam', loss='mse')

# Fit the model
model.fit(X_train, y_train, epochs=100, verbose=1, validation_data=(X_test, y_test))

# Evaluate the model on the test set
loss = model.evaluate(X_test, y_test, verbose=0)
print(f"Test Loss: {loss}")

# Make predictions on a new sequence
x_input = array(raw_sequence[-steps:])
x_input = x_input.reshape((1, steps, features))
y_pred = model.predict(x_input, verbose=0)

print("*************************************************")
print(f"The next predicted value is ")
print(y_pred)
print(f"The next true value is ")
print(raw_sequence.iloc[-1])
print("*************************************************")